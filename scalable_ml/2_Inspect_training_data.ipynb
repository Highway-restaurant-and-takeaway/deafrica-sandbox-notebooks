{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting training data <img align=\"right\" src=\"../Supplementary_data/DE_Africa_Logo_Stacked_RGB_small.jpg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Prior to training a machine learning classifier, it can be useful to understand which  of our feature layers are most useful for distinguishing between classes. The feature layers the model is trained on form the **knowledge base** of the algorithm. We can explore this knowledge base using class-specific [violin plots](https://en.wikipedia.org/wiki/Violin_plot#:~:text=A%20violin%20plot%20is%20a,by%20a%20kernel%20density%20estimator.), and through a dimensionality reduction approach called [principal-components analysis](https://builtin.com/data-science/step-step-explanation-principal-component-analysis). The latter transforms our large dataset with lots of variables into a smaller dataset with fewer variables (while still preserving much of the variance), this allows us to visualise a very complex dataset in a relatively intuitive and straightforward manner.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "Using the training data written to file in the previous notebook, [1_Extract_training_data](1_Extract_training_data.ipynb), this notebook will:\n",
    "\n",
    "1. Plot class-specific violin plots for each of the feature layers in the training data,\n",
    "2. Calculate the first two prinicpal components of the dataset and plot them as a scatter plot.\n",
    "3. Calculate the first three prinicpal components of the dataset and plot them as a 3D scatter plot.\n",
    "\n",
    "> This notebook is provided as an exploratory means to better understand the dataset we're working with. It does not produce any outputs that are required for subsequent notebooks.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, chi2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Parameters\n",
    "\n",
    "* `training_data`: Name and location of the training data `.txt` file output from runnning `1_Extract_training_data.ipynb`\n",
    "* `class_dict`: A dictionary mapping the 'string' name of the classes to the integer values that represent our classes in the training data (e.g. `{'crop': 1., 'noncrop': 0.}`)\n",
    "* `field`: This is the name of column in the original training data shapefile that contains the class labels. This is provided simply so we can remove this attribute before we plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = \"results/training_data/test_training_data.txt\"\n",
    "\n",
    "class_dict = {'crop':1, 'noncrop':0}\n",
    "\n",
    "field = 'class'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "model_input = np.loadtxt(training_data)\n",
    "\n",
    "# load the column_names\n",
    "with open(training_data, 'r') as file:\n",
    "    header = file.readline()\n",
    "    \n",
    "column_names = header.split()[1:]\n",
    "\n",
    "# Extract relevant indices from training data\n",
    "model_col_indices = [column_names.index(var_name) for var_name in column_names[1:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling\n",
    "\n",
    "This cell extract each class in the training data array and assigns it to a dictionary, this step will allow for cleanly plotting our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "for key, value in class_dict.items():\n",
    "    print(key, value)\n",
    "    # extract values for class from training data\n",
    "    arr = model_input[model_input[:,0]==value]\n",
    "    # create a pandas df for ease of use later\n",
    "    df = pd.DataFrame(arr).rename(columns={i:column_names[i] for i in range(0,len(column_names))}).drop(field, axis=1)\n",
    "    # Scale the dataframe\n",
    "    scaled_df = StandardScaler(with_mean=False).fit_transform(df)\n",
    "    scaled_df = pd.DataFrame(scaled_df, columns=df.columns)\n",
    "    \n",
    "    dfs.update({key:scaled_df})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature layer violin plots\n",
    "\n",
    "TODO:\n",
    "- Not sure if the automation of the offsets for the violin plots will work when num of classes in the dataset is >2..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a random list of colors same length as num of classes \n",
    "get_colors = lambda n: list(map(lambda i: \"#\" + \"%06x\" % random.randint(0, 0xFFFFFF),range(n)))\n",
    "colors = get_colors(len(dfs))\n",
    "\n",
    "#generate list of offsets & widths for plotting\n",
    "start=-0.2 \n",
    "end=0.2\n",
    "offsets = list(np.linspace(start,end,len(dfs)))\n",
    "if len(dfs) == 2:\n",
    "    width=0.4\n",
    "else:\n",
    "    width=np.abs(offsets[0] - offsets[1])\n",
    "\n",
    "#create figure and axes \n",
    "fig, ax = plt.subplots(figsize=(40,8))\n",
    "\n",
    "for key, color, offset in zip(dfs,colors, offsets):\n",
    "    #create violin plots\n",
    "    pp = ax.violinplot(dfs[key].values,\n",
    "                       showmedians=True,\n",
    "                       positions=np.arange(dfs[key].values.shape[1])+offset, widths=width\n",
    "                      )\n",
    "    # change the colour of the plots\n",
    "    for pc in pp['bodies']:\n",
    "        pc.set_facecolor(color)\n",
    "        pc.set_edgecolor(color)\n",
    "        pc.set_alpha(1)\n",
    "    \n",
    "    #change the line style in the plots\n",
    "    for partname in ('cbars','cmins','cmaxes','cmedians'):\n",
    "        vp = pp[partname]\n",
    "        vp.set_edgecolor('black')\n",
    "        vp.set_linewidth(1)\n",
    "\n",
    "#tidy the plot, add a title and legend\n",
    "ax.set_xticks(np.arange(len(column_names[1:])))\n",
    "ax.set_xticklabels(column_names[1:])\n",
    "# ax.set_ylim(-5,12)\n",
    "ax.set_xlim(-0.5,len(column_names[1:])-.5)\n",
    "ax.set_ylabel(\"Scaled Values\", fontsize=14)\n",
    "ax.set_xlabel(\"Feature Layers\", fontsize=14)\n",
    "ax.set_title(\"Training Data Knowledge-Base\", fontsize=14)\n",
    "ax.legend([Patch(facecolor=c) for c in colors], [key for key in dfs], loc='upper right');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "Feature Selection is an important part of supervised machine learning. Model's can become unwieldy or have poor predictive capacity when there are many features because:\n",
    "\n",
    "1. Training time increases exponentially with number of features.\n",
    "2. Models have increasing risk of overfitting with increasing number of features.\n",
    "\n",
    "Feature Selection methods help with these problems by reducing the dimensions without much loss of the total information. There are many ways feature layers can be selected, scikit-learn's [feature_selection](https://scikit-learn.org/stable/modules/feature_selection.html) module has good documentation on the various methods.\n",
    "\n",
    "Below, we demonstrate feature selection using the [f_classif](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html) function which selects features based on their linear correlation with the classsification label.  The [mutual_info_classif](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html) is another method that measures the dependence of one variable to another.\n",
    "\n",
    "* `num_of_features`: This parameters controls the number of features returned after feature selection.\n",
    "\n",
    "* `selection_method`: The statistical method used to select the most discriminating features, options that will work with the code below include `mutual_info_classif`, `f_classif`, and `chi2` (chi2 will only work if all your features are non-negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_features = 15\n",
    "\n",
    "selection_method = mutual_info_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the features\n",
    "selector = SelectKBest(selection_method, k=num_of_features)\n",
    "selected_features = selector.fit_transform(model_input[:, model_col_indices], model_input[:, 0])\n",
    "print(selected_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the name of the features selected\n",
    "idx_of_selections = selector.get_support(indices=True)\n",
    "selected_columns=list(np.array(column_names[1:])[idx_of_selections])\n",
    "\n",
    "print('Features selected:')\n",
    "print(selected_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recreate our `model_input` and `column_name` parameters, using only the selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = np.append(model_input[:,0].reshape(len(model_input[:,0]), 1), selected_features, axis=1)\n",
    "print(model_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns.insert(0, column_names[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "The code below will calculate and plot the first two principal components of our training dataset. \n",
    "\n",
    "The first step is to standardise our data to express each feature layer in terms of mean and standard deviation, this is necessary because principal component analysis is quite sensitive to the variances of the initial variables. If there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. So, transforming the data to comparable scales can prevent this problem.  We do this using sklearn's `StandardScalar` function which will normalise the values in an array to the array's mean and standard deviation via the formuala: `z = (x-u/s)`, where `u` is the mean of and `s` is the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean and variance for each feature\n",
    "x = StandardScaler().fit_transform(model_input[:,1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conduct the PCAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#two components\n",
    "pca2 = PCA(n_components=2)\n",
    "pca2_fit = pca2.fit_transform(x)\n",
    "#three PCA components\n",
    "pca3 = PCA(n_components=3)\n",
    "pca3_fit = pca3.fit_transform(x)\n",
    "\n",
    "#add back to df\n",
    "pca2_df = pd.DataFrame(data = pca2_fit,\n",
    "                      columns = ['PC1', 'PC2'])\n",
    "pca3_df = pd.DataFrame(data = pca3_fit,\n",
    "                      columns = ['PC1', 'PC2', 'PC3'])\n",
    "\n",
    "# concat with classes\n",
    "result2 = pd.concat([pca2_df, pd.DataFrame({'class':model_input[:,0]})], axis=1)\n",
    "result3 = pd.concat([pca3_df, pd.DataFrame({'class':model_input[:,0]})], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2,b2 = pca2.explained_variance_ratio_\n",
    "a3,b3,c3 = pca3.explained_variance_ratio_\n",
    "print(\"Variance explained by two principal components = \" + str(round((a2+b2)*100, 2))+\" %\")\n",
    "print(\"Variance explained by three principal components = \" + str(round((a3+b3+c3)*100, 2))+\" %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot both 2 & 3 principal components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(result2) > 5000:\n",
    "    result2=result2.sample(n=5000)\n",
    "if len(result3) > 5000:\n",
    "    result3=result3.sample(n=5000)\n",
    "\n",
    "fig = plt.figure(figsize=(18,8))\n",
    "fig.suptitle('Training data: Principal Component Analysis', fontsize=14)\n",
    "\n",
    "# First subplot\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "scatter1=sns.scatterplot(x=\"PC1\", y=\"PC2\",\n",
    "           data=result2,\n",
    "           hue='class',\n",
    "           hue_norm=tuple(np.unique(result2['class'])),\n",
    "           palette='viridis',\n",
    "           legend=False,\n",
    "           alpha=0.5,                    \n",
    "           ax=ax\n",
    "          )\n",
    "\n",
    "ax.set_title('Two Principal Components', fontsize=14)\n",
    "ax.grid(True)\n",
    "\n",
    "# Second subplot\n",
    "ax = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "scatter2 = ax.scatter(result3['PC1'], result3['PC2'], result3['PC3'],\n",
    "                      c=result3['class'], s=60, alpha=0.5)\n",
    "\n",
    "# make simple, bare axis lines through space:\n",
    "xAxisLine = ((min(result3['PC1']), max(result3['PC1'])), (0, 0), (0,0))\n",
    "ax.plot(xAxisLine[0], xAxisLine[1], xAxisLine[2], 'r')\n",
    "yAxisLine = ((0, 0), (min(result3['PC2']), max(result3['PC2'])), (0,0))\n",
    "ax.plot(yAxisLine[0], yAxisLine[1], yAxisLine[2], 'r')\n",
    "zAxisLine = ((0, 0), (0,0), (min(result3['PC3']), max(result3['PC3'])))\n",
    "ax.plot(zAxisLine[0], zAxisLine[1], zAxisLine[2], 'r')\n",
    "\n",
    "ax.set_title('Three Principal Components', fontsize=14)\n",
    "ax.set_xlabel(\"PC1\")\n",
    "ax.set_ylabel(\"PC2\")\n",
    "ax.set_zlabel(\"PC3\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export training data\n",
    "\n",
    "Once we've collected all the training data we require, we can write the data to disk. This will allow us to import the data in the next step(s) of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the name and location of the output file\n",
    "output_file = \"results/training_data/test_training_data.txt\"\n",
    "#Export files to disk\n",
    "np.savetxt(output_file, model_input, header=\" \".join(selected_columns), fmt=\"%4f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommended next steps\n",
    "\n",
    "To continue working through the notebooks in this `Scalable Machine Learning on the ODC` workflow, go to the next notebook `3_Train_fit_evaluate_classifier.ipynb`.\n",
    "\n",
    "1. [Extracting_training_data](1_Extracting_training_data.ipynb) \n",
    "2. **Inspect_training_data (this notebook)**\n",
    "3. [Train_fit_evaluate_classifier](3_Train_fit_evaluate_classifier.ipynb)\n",
    "4. [Predict](4_Predict.ipynb)\n",
    "5. [Accuracy_assessment](5_Accuracy_assessment.ipynb)\n",
    "6. [Object-based_filtering](6_Object-based_filtering_(optional).ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Additional information\n",
    "\n",
    "**License:** The code in this notebook is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \n",
    "Digital Earth Africa data is licensed under the [Creative Commons by Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "\n",
    "**Contact:** If you need assistance, please post a question on the [Open Data Cube Slack channel](http://slack.opendatacube.org/) or on the [GIS Stack Exchange](https://gis.stackexchange.com/questions/ask?tags=open-data-cube) using the `open-data-cube` tag (you can view previously asked questions [here](https://gis.stackexchange.com/questions/tagged/open-data-cube)).\n",
    "If you would like to report an issue with this notebook, you can file one on [Github](https://github.com/digitalearthafrica/deafrica-sandbox-notebooks).\n",
    "\n",
    "**Last modified:** August 2020\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tags\n",
    "Browse all available tags on the DE Africa User Guide's [Tags Index](https://) (placeholder as this does not exist yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
