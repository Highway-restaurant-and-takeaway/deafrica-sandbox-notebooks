{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting training data from the ODC <img align=\"right\" src=\"../Supplementary_data/DE_Africa_Logo_Stacked_RGB_small.jpg\">\n",
    "\n",
    "* **Products used:** \n",
    "[ga_ls8c_gm_2_annual](https://explorer.digitalearth.africa/ga_ls8c_gm_2_annual), [s2_l2a](https://explorer.digitalearth.africa/s2_l2a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "**Training data** is the most important part of any supervised machine learning workflow. The quality of the training data has a greater impact on the classification than the algorithm used. Large and accurate training data sets are preferable: increasing the training sample size results in increased classification accuracy ([Maxell et al 2018](https://www.tandfonline.com/doi/full/10.1080/01431161.2018.1433343)).  A review of training data methods in the context of Earth Observation is available [here](https://www.mdpi.com/2072-4292/12/6/1034) \n",
    "\n",
    "When creating training data, be sure to capture the **spectral variability** of the class, and to use imagery from the time period you want to classify (rather than relying on basemap composites). Another common problem with training data is **class imbalance**. This can occur when one of your classes is relatively rare and therefore the rare class will comprise a smaller proportion of the training set. When imbalanced data is used, it is common that the final classification will under-predict less abundant classes relative to their true proportion.\n",
    "\n",
    "There are many platforms to use for gathering training data, the best one to use depends on your application. GIS platforms are great for collection training data as they are highly flexible and mature platforms; [Geo-Wiki](https://www.geo-wiki.org/) and [Collect Earth Online](https://collect.earth/home) are two open-source websites that may also be useful depending on the reference data strategy employed.\n",
    "\n",
    "This notebook assumes the user has already collected reference data polygons, and now wishes to extract feature layers from the `open-data-cube` to assist in classifying satellite images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "This notebook will extract training data (feature layers) from the `open-data-cube` using geometries within a shapefile (or geojson). To do this, we rely on a custom `deafrica-sandbox-notebooks` function called `collect_training_data`, contained within the [deafrica_classificationtools](../Scripts/deafrica_classificationtools.py) script.  The goal of this notebook is to familarise users with this function so they can extract the appropriate data for their use-case.\n",
    "\n",
    "1. Preview the polygons in our training data by plotting them on a basemap\n",
    "2. Extract training data from the datacube using `collect_training_data`'s inbuilt feature layer parameters\n",
    "3. Extract training data from the datacube using  a custom defined feature layer function that we can pass to `collect_training_data`\n",
    "4. Export the training data to disk for use in subsequent scripts\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "To run this analysis, run all the cells in the notebook, starting with the \"Load packages\" cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install richdem\n",
    "# !pip install https://packages.dea.ga.gov.au/hdstats/hdstats-0.1.5.tar.gz\n",
    "# !pip install dask-ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/env/lib/python3.6/site-packages/datacube/storage/masking.py:4: DeprecationWarning: datacube.storage.masking has moved to datacube.utils.masking\n",
      "  category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "import datacube\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import subprocess as sp\n",
    "import geopandas as gpd\n",
    "from datacube.utils.geometry import assign_crs\n",
    "\n",
    "sys.path.append('../Scripts')\n",
    "from deafrica_plotting import map_shapefile\n",
    "from xr_geomedian_tmad import xr_geomedian_tmad\n",
    "from deafrica_bandindices import calculate_indices\n",
    "from deafrica_classificationtools import collect_training_data \n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# from deafrica_temporal_statistics import temporal_statistics\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis parameters\n",
    "\n",
    "* `path`: The path to the input shapefile from which we will extract training data. A default shapefile is provided.\n",
    "* `field`: This is the name of column in your shapefile attribute table that contains the class labels. **The class labels must be integers**\n",
    "* `ncpus`: Set this value to > 1 to parallize the collection of training data. eg. npus=8. \n",
    "\n",
    "> **Note**: With supervised classification, its common to have many, many labelled geometries in the training data. `collect_training_data` can parallelize across the geometries in order to speed up the extracting of training data. Setting `ncpus>1` will automatically trigger the parallelization, however, its best to set `ncpus=1` to begin with to assist with debugging before triggering the parallelization. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/training/test_training_dataset.shp' \n",
    "field = 'class'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the number of cpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ncpus = 15\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ncpus = int(float(sp.getoutput('env | grep CPU')[-4:]))\n",
    "except:\n",
    "    ncpus = int(float(sp.getoutput('env | grep CPU')[-3:]))\n",
    "\n",
    "print('ncpus = '+str(ncpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview input data\n",
    "\n",
    "We can load and preview our input data shapefile using `geopandas`. The shapefile should contain a column with class labels (e.g. 'class'). These labels will be used to train our model. \n",
    "\n",
    "> Remember, the class labels **must** be represented by `integers`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>POLYGON ((38.52475 12.27793, 38.52475 12.27841...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>POLYGON ((35.27410 5.60795, 35.27410 5.60842, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>POLYGON ((38.97103 -1.84843, 38.97103 -1.84796...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>POLYGON ((38.30430 0.32585, 38.30430 0.32632, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>POLYGON ((41.33019 3.89073, 41.33019 3.89120, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                                           geometry\n",
       "0      0  POLYGON ((38.52475 12.27793, 38.52475 12.27841...\n",
       "1      0  POLYGON ((35.27410 5.60795, 35.27410 5.60842, ...\n",
       "2      0  POLYGON ((38.97103 -1.84843, 38.97103 -1.84796...\n",
       "3      0  POLYGON ((38.30430 0.32585, 38.30430 0.32632, ...\n",
       "4      0  POLYGON ((41.33019 3.89073, 41.33019 3.89120, ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load input data shapefile\n",
    "input_data = gpd.read_file(path)\n",
    "\n",
    "# Plot first five rows\n",
    "input_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training data in an interactive map\n",
    "# map_shapefile(input_data, attribute=field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting training data\n",
    "\n",
    "The function `collect_training_data` takes our shapefile containing class labels and extracts training data from the datacube over the locations specified by the input geometries. The function will also pre-process our training data by stacking the arrays into a useful format and removing an `NaN` (not-a-number) values. \n",
    "\n",
    "`Collect_training_data` has the ability to generate many different types of **feature layers**, relatively simple layers can be calculated using pre-defined parameters within the function; more complex layers can be computed by passing in a `custom_func`. To begin with, let's try generating feature layers using the pre-defined methods.\n",
    "\n",
    "The in-built feature layer parameters are described below:\n",
    "* `product`: The name of the product to extract from the datacube. In this example we use a geomedian composite from 2018, `'ga_ls8c_gm_2_annual'`\n",
    "* `time`: The time range from which to extract data\n",
    "* `calc_indices`: This parameter provides a method for calculating a number of remote sensing indices (e.g. `['NDWI', 'NDVI']`). Any of the indices found in the [deafrica_bandindices](../Scripts/deafrica_bandindices.py) script can be used here\n",
    "* `drop`: If this variable is set to True, and 'calc_indices' are supplied, the spectral bands will be dropped from the dataset leaving only the band indices as data variables in the dataset.\n",
    "* `reduce_func`: The classification models we're applying here require our training data to be in two dimensions (ie. `x` & `y`). If our data has a time-dimension (e.g. if we load in an annual time-series of satellite images) then we need to collapse the time dimension.  \"reduce_func\" is simply the summary statistic used to collapse the temporal dimension. Options are 'mean', 'median', 'std', 'max', 'min', 'geomedian'. In the default example we are loading a geomedian composite, so there is no time dimension to reduce.\n",
    "* `zonal_stats`: An optional string giving the names of zonal statistics to calculate across each polygon. Default is None (all pixel values are returned). Supported values are 'mean', 'median', 'max', 'min', and 'std'.\n",
    "* `return_coords` : If `True`, then the traiining data will contain two extra columns 'x_coord' and 'y_coord' corresponding to the x,y coordinate of each sample. This variable can be useful for handling spatial autocorrelation between samples later on in the ML workflow. \n",
    "\n",
    "In addition to the parameters required for `collect_training_data`, we also need to set up a few parameters for the open-data-cube query, such as `measurements` (the bands to load from the satellite), the `resolution` (the cell size), and the `output_crs` (the output projection). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up our inputs to collect_training_data\n",
    "products =  ['ga_ls8c_gm_2_annual']\n",
    "time = ('2018')\n",
    "reduce_func = None\n",
    "calc_indices = ['NDVI','LAI', 'NDWI']\n",
    "drop = False\n",
    "zonal_stats = 'mean'\n",
    "return_coords = True\n",
    "\n",
    "# Set up the inputs for the ODC query\n",
    "measurements =  ['blue','green','red','nir','swir_1','swir_2']\n",
    "resolution = (-30,30)\n",
    "output_crs='epsg:6933'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a datacube query object\n",
    "query = {\n",
    "    'time': time,\n",
    "    'measurements': measurements,\n",
    "    'resolution': resolution,\n",
    "    'output_crs': output_crs,\n",
    "    'group_by' : 'solar_day',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the `collect_training_data` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names, model_input = collect_training_data(\n",
    "                                    gdf=input_data,\n",
    "                                    products=products,\n",
    "                                    dc_query=query,\n",
    "                                    ncpus=ncpus,\n",
    "                                    return_coords=return_coords,\n",
    "                                    field=field,\n",
    "                                    calc_indices=calc_indices,\n",
    "                                    reduce_func=reduce_func,\n",
    "                                    drop=drop,\n",
    "                                    zonal_stats=zonal_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function returns two numpy arrays, the first (`column_names`) contains a list of the names of the feature layers we've computed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second array (`model_input`) contains the data from our labelled geometries. The first item in the array is the class integer (e.g. in the default example 1. 'crop', or 0. 'noncrop'), the second set of items are the values for each feature layer we computed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array_str(model_input, precision=2, suppress_small=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Custom feature layers\n",
    "\n",
    "The feature layers that are most relevant for discriminating the classes of your classification problem may be more complicated than those provided in the `collect_training_data` function.  In this case, we can pass a custom feature layer function through the `custom_func` parameter.   \n",
    "\n",
    "* `custom_func`: A custom function for generating feature layers. If this parameter is set, all other options (excluding 'zonal_stats'), will be ignored. The result of the 'custom_func' must be a single xarray dataset containing 2D coordinates (i.e x, y; no time dimension). The custom function has access to the datacube dataset extracted using the `dc_query` params. To load other datasets, you can use the `like=ds.geobox` parameter in `dc.load`\n",
    "\n",
    "First, lets define a custom feature layer function that will demonstrate how to extract more complicated temporal statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_feature_layers import xr_terrain\n",
    "\n",
    "def two_seasons_gm_mads(ds):\n",
    "    dc = datacube.Datacube(app='training')\n",
    "    ds = ds / 10000\n",
    "    ds1 = ds.sel(time=slice('2019-01', '2019-06'))\n",
    "    ds2 = ds.sel(time=slice('2019-07', '2019-12')) \n",
    "    \n",
    "    def fun(ds, era):\n",
    "        \n",
    "        #geomedian and tmads\n",
    "        gm_mads = xr_geomedian_tmad(ds)\n",
    "        gm_mads = calculate_indices(gm_mads,\n",
    "                               index=['NDVI', 'LAI'],\n",
    "                               drop=False,\n",
    "                               normalise=False,\n",
    "                               collection='s2')\n",
    "        \n",
    "        gm_mads['edev'] = -np.log(gm_mads['edev'])\n",
    "        gm_mads['sdev'] = -np.log(gm_mads['sdev'])\n",
    "        gm_mads['bcdev'] = -np.log(gm_mads['bcdev'])\n",
    "        \n",
    "        for band in gm_mads.data_vars:\n",
    "            gm_mads = gm_mads.rename({band:band+era})\n",
    "        \n",
    "        return gm_mads\n",
    "    \n",
    "    epoch1 = fun(ds1, era='_S1')\n",
    "    epoch2 = fun(ds2, era='_S2')\n",
    "    \n",
    "#     from datacube.testutils.io import rio_slurp_xarray\n",
    "#     slope = rio_slurp_xarray(\"https://deafrica-data.s3.amazonaws.com/ancillary/dem-derivatives/cog_slope_africa.tif\", gbox=ds.geobox)\n",
    "    slope = dc.load(product='srtm', like=ds.geobox).squeeze()\n",
    "    slope = slope.elevation\n",
    "    slope = xr_terrain(slope, 'slope_riserun')\n",
    "    slope = slope.to_dataset(name='slope')\n",
    "    \n",
    "    result = xr.merge([epoch1,\n",
    "                       epoch2,\n",
    "                       slope],compat='override')\n",
    "\n",
    "    return result.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can pass this function to `collect_training_data`.  For each of the geometries in our shapefile we will extract temporal statistics, and elevation data.\n",
    "\n",
    "Because we are now interested in calculating a range of temporal statistics, we will redefine our intial parameters to include a time-series of Sentinel-2 data (whereas above we loaded data from a geomedian composite).  Remember, passing in a `custom_func` to `collect_training_data` means many of the other feature layer parameters are ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up our inputs to collect_training_data\n",
    "products =  ['s2_l2a']\n",
    "time = ('2019-01','2019-12')\n",
    "zonal_stats = 'median' \n",
    "return_coords=True\n",
    "\n",
    "# Set up the inputs for the ODC query\n",
    "measurements =  ['red','blue','green','nir','swir_1']\n",
    "resolution = (-20,20)\n",
    "output_crs='epsg:6933'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a new datacube query object\n",
    "query = {\n",
    "    'time': time,\n",
    "    'measurements': measurements,\n",
    "    'resolution': resolution,\n",
    "    'output_crs': output_crs,\n",
    "    'group_by' : 'solar_day',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are now extracting 12-months of Sentinel-2 data from the datacube for every geometry, the load times will be much slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "crops = input_data[input_data['class']==1].sample(n=100, replace=False).reset_index(drop=True)\n",
    "noncrops = input_data[input_data['class']==0].sample(n=100, replace=False).reset_index(drop=True)\n",
    "\n",
    "import pandas as pd\n",
    "input_data = pd.concat([crops,noncrops]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing data using user supplied custom function\n",
      "Taking zonal statistic: median\n",
      "Collecting training data in parallel mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 196/200 [10:38<00:13,  3.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output training data has shape (196, 24)\n",
      "Removed NaNs & Infs, cleaned input shape:  (163, 24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "column_names, model_input = collect_training_data(\n",
    "                                    gdf=input_data,\n",
    "                                    products=products,\n",
    "                                    dc_query=query,\n",
    "                                    ncpus=ncpus,\n",
    "                                    return_coords=return_coords,\n",
    "                                    field=field,\n",
    "                                    zonal_stats=zonal_stats,\n",
    "                                    custom_func=two_seasons_gm_mads\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['class', 'red_S1', 'blue_S1', 'green_S1', 'nir_S1', 'swir_1_S1', 'edev_S1', 'sdev_S1', 'bcdev_S1', 'NDVI_S1', 'LAI_S1', 'red_S2', 'blue_S2', 'green_S2', 'nir_S2', 'swir_1_S2', 'edev_S2', 'sdev_S2', 'bcdev_S2', 'NDVI_S2', 'LAI_S2', 'slope', 'x_coord', 'y_coord']\n",
      "\n",
      "[[      1.         0.07       0.04 ...       0.29 3464870.   -406010.  ]\n",
      " [      1.         0.13       0.07 ...       0.07 3377000.    236650.  ]\n",
      " [      1.         0.08       0.04 ...       0.11 3432470.   -267270.  ]\n",
      " ...\n",
      " [      0.         0.08       0.05 ...       0.46 3695990.   -593590.  ]\n",
      " [      0.         0.16       0.08 ...       0.03 4083460.   1250110.  ]\n",
      " [      0.         0.2        0.08 ...       0.04 3648150.    110110.  ]]\n"
     ]
    }
   ],
   "source": [
    "print(column_names)\n",
    "print('')\n",
    "print(np.array_str(model_input, precision=2, suppress_small=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperate coordinates\n",
    "\n",
    "By setting `return_coords=True` in the `collect_training_data` function, our training data now has two extra columns called `x_coord` and `y_coord`.  We need to seperate these from our training dataset as they will not be used to train the machine learning model. Instead, these variables will be used to help conduct Spatial K-fold Cross validation (SKVC) and spatially aware test-train-splits in the notebook `3_Train_fit_evaluate_classifier`.  For more information on why this is important, see this [article](https://www.tandfonline.com/doi/abs/10.1080/13658816.2017.1346255?journalCode=tgis20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the variables we want to use to train our model\n",
    "coord_variables = ['x_coord', 'y_coord']\n",
    "\n",
    "# Extract relevant indices from the processed shapefile\n",
    "model_col_indices = [column_names.index(var_name) for var_name in coord_variables]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"results/training_data/training_data_coordinates.txt\", model_input[:, model_col_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export training data\n",
    "\n",
    "Once we've collected all the training data we require, we can write the data to disk. This will allow us to import the data in the next step(s) of the workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the name and location of the output file\n",
    "output_file = \"results/training_data/test_training_data.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab all columns except the x-y coords\n",
    "model_col_indices = [column_names.index(var_name) for var_name in column_names[0:-2]]\n",
    "#Export files to disk\n",
    "np.savetxt(output_file, model_input[:, model_col_indices], header=\" \".join(column_names[0:-2]), fmt=\"%4f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommended next steps\n",
    "\n",
    "To continue working through the notebooks in this `Scalable Machine Learning on the ODC` workflow, go to the next notebook `2_Inspect_training_data.ipynb`.\n",
    "\n",
    "1. **Extracting_training_data (this notebook)**\n",
    "2. [Inspect_training_data](2_Inspect_training_data.ipynb)\n",
    "3. [Train_fit_evaluate_classifier](3_Train_fit_evaluate_classifier.ipynb)\n",
    "4. [Predict](4_Predict.ipynb)\n",
    "5. [Accuracy_assessment](5_Accuracy_assessment.ipynb)\n",
    "6. [Object-based_filtering](6_Object-based_filtering_(optional).ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Additional information\n",
    "\n",
    "**License:** The code in this notebook is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \n",
    "Digital Earth Africa data is licensed under the [Creative Commons by Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "\n",
    "**Contact:** If you need assistance, please post a question on the [Open Data Cube Slack channel](http://slack.opendatacube.org/) or on the [GIS Stack Exchange](https://gis.stackexchange.com/questions/ask?tags=open-data-cube) using the `open-data-cube` tag (you can view previously asked questions [here](https://gis.stackexchange.com/questions/tagged/open-data-cube)).\n",
    "If you would like to report an issue with this notebook, you can file one on [Github](https://github.com/digitalearthafrica/deafrica-sandbox-notebooks).\n",
    "\n",
    "**Last modified:** August 2020\n",
    "\n",
    "**Compatible datacube version:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datacube.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tags\n",
    "Browse all available tags on the DE Africa User Guide's [Tags Index](https://) (placeholder as this does not exist yet)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "**Tags**:  :index:`Sentinel-2`, :index:`deafrica_classificationtools`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
