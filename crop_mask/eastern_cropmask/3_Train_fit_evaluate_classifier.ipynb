{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, fit, and evaluate classifier <img align=\"right\" src=\"../Supplementary_data/DE_Africa_Logo_Stacked_RGB_small.jpg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "1. Split the training data into a training set and a test set\n",
    "2. Optional: Standardise the datasets\n",
    "2. Train a classifier\n",
    "3. Evaluate the classifier using a number of methods\n",
    "4. Optimise the model hyperparameters using GridSearchCV\n",
    "5. Retrain the model using the optimised hyperparameters\n",
    "6. Re-evaluate the classifier\n",
    "7. Optionally plot Receiver Operating Characteristic (ROC) Curves (for binary classification only)\n",
    "6. Save model to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install dask-ml\n",
    "# !pip freeze | grep hdstats\n",
    "# !pip install https://packages.dea.ga.gov.au/hdstats/hdstats-0.1.6.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/env/lib/python3.6/site-packages/datacube/storage/masking.py:4: DeprecationWarning: datacube.storage.masking has moved to datacube.utils.masking\n",
      "  category=DeprecationWarning)\n",
      "/env/lib/python3.6/site-packages/osgeo/gdal.py:107: DeprecationWarning: gdal.py was placed in a namespace, it is now available as osgeo.gdal\n",
      "  DeprecationWarning)\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# -- scikit-learn classifiers, uncomment the one of interest----\n",
    "\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import dump\n",
    "import subprocess as sp\n",
    "import dask.array as da\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, make_scorer\n",
    "\n",
    "sys.path.append('../../Scripts')\n",
    "from deafrica_classificationtools import spatial_clusters, SKCV, spatial_train_test_split\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Parameters\n",
    "\n",
    "* `training_data`: Name and location of the training data `.txt` file output from runnning `1_Extract_training_data.ipynb`\n",
    "* `Classifier`: This parameter refers to the scikit-learn classification model to use, first uncomment the classifier of interest in the `Load Packages` section and then enter the function name into this parameter `e.g. Classifier = SVC`   \n",
    "* `class_dict`: A dictionary mapping the 'string' name of the classes to the integer values that represent our classes in the training data (e.g. `{'crop': 1., 'noncrop': 0.}`)\n",
    "* `metric` : A single string that denotes the scorer used to find the best parameters for refitting the estimator to evaluate the predictions on the test set. See the scoring parameter page [here](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter) for a pre-defined list of options.\n",
    "* `cv_splits` : Determines the number of k-fold cross-validations to conduct during testing of the model.  A higher number will reduce the possibility of over-fitting, but will require more time to compute. 5-10 is a good default number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = \"results/training_data/test_training_data.txt\"\n",
    "coordinate_data = \"results/training_data/training_data_coordinates.txt\"\n",
    "\n",
    "Classifier = RandomForestClassifier\n",
    "\n",
    "class_dict = {'crop':1, 'noncrop':0}\n",
    "\n",
    "metric = 'f1' #good for binary classifications\n",
    "\n",
    "cv_splits = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatically find the number of cpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ncpus = 2\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ncpus = int(float(sp.getoutput('env | grep CPU')[-4:]))\n",
    "except:\n",
    "    ncpus = int(float(sp.getoutput('env | grep CPU')[-3:]))\n",
    "\n",
    "print('ncpus = '+str(ncpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import training  and coordinate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "model_input = np.loadtxt(training_data)\n",
    "coordinates = np.loadtxt(coordinate_data)\n",
    "\n",
    "# load the column_names\n",
    "with open(training_data, 'r') as file:\n",
    "    header = file.readline()\n",
    "    \n",
    "column_names = header.split()[1:]\n",
    "\n",
    "# Extract relevant indices from training data\n",
    "model_col_indices = [column_names.index(var_name) for var_name in column_names[1:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split training and testing data\n",
    "\n",
    "Here will split the training data into four seperate datasets:\n",
    "* `train_features` : the feature layer data we will train the classifier on\n",
    "* `test_features` :  the feature layer data we will use to test the accuracy/precision of our classifier\n",
    "* `train_labels` : the dependent variables we will train the classifier on (in the default example the integers that represent the classes i.e. 1. and 0.)\n",
    "* `test_labels` : the dependent variables we will use to test the accuracy/precision of our classifier.\n",
    "\n",
    "\n",
    "The deafrica function `spatial_train_test_split` is a custom function similar to `sklearn.model_selection.train_test_split` but instead works on spatial coordinate data.  Coordinate data is grouped according to either a Guassian Mixture, KMeans, or Heirachical clustering\n",
    "algorithm. Grouping by spatial clusters is preferred over plain random splits for spatial data to avoid overestimating validation scores due to the inherent autocorrelation of spatial data. To use `spatial_train_test_split` we need to set more parameters: \n",
    "\n",
    "* `test_size` : This will determine what fraction of the dataset will be set aside as the testing dataset. There is a trade-off here between having a larger test set that will help us better determine the quality of our classifier, and leaving enough data to train the classifier. A good deafult is to set 10-20 % of your dataset aside for testing purposes.\n",
    "* `cluster_method` : Which algorithm to use to create spatial groups, either `'Hierarchical'`, `'GMM'` or `'KMeans'`. Key word arguments for these algorithms can also be passed to the function if non-default behaviour is required. See the docs for [GMM](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html), [Kmeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) and [Heirarchical](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html) for options. \n",
    "* `max_distance`:  This parameter is used when using the 'Heirarchical' clustering method. The maximum distance describes the maximum euclidean distances between all observations in a cluster. The units of distance depend on the map projection of the coordinate data. e.g if using a UTM projection, then the `max_distance` is in metres, so to set the maximum distance of each cluster to 200 km, `max_distance=200000`. \n",
    "* `n_clusters` : Number of spatial clusters to create using the coordinate values of the training data samples. This option only applies if using the 'GMM' or 'Kmeans' clustering methods, if using the 'Heirarchical' method then the number of clusters is determined automatically using the maximum distance threshold.\n",
    "* `kfold_method` : Which stratgey to use to split to the data. One of either `'SpatialShuffleSplit'` or `'SpatialKFold'`.\n",
    "* `balance` : if setting `kfold_method` to `'SpatialShuffleSplit'` this should be an integer (10 is a good deafult) that represents the number of splits generated per iteration to try to balance the amount of data in each set so that *test_size* and *train_size* are respected. If setting `kfold_method` to `'SpatialKFold'` then this value should be either `True` or `False`.  If False, each fold will have the same number of clusters (which can have different number of data points in them).\n",
    "\n",
    "> Note: If you require setting keyword arguments to use the non-default setting of the clustering algorithms, then you will need to manually enter these parameters wherever the `SKCV` and `spatial_test_train_split` functions are called.  For example, if you want to set the `cluster_method` to `'GMM'` but then you'd like to alter the covariance type, then you'll need to add `covariance_type = 'tied'` to the functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.15\n",
    "\n",
    "cluster_method = 'Hierarchical'\n",
    "\n",
    "max_distance = 200000\n",
    "\n",
    "n_clusters=None\n",
    "\n",
    "kfold_method = 'SpatialShuffleSplit'\n",
    "\n",
    "balance = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After setting the parameters, lets first generate spatial clusters to visualize how our data will be grouped.  You may want to refine the parameters to achieve a grouping that works for your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create clustes\n",
    "spatial_groups = spatial_clusters(coordinates=coordinates,\n",
    "                                  method=cluster_method,\n",
    "                                  max_distance=max_distance,\n",
    "                                  n_groups=n_clusters)\n",
    "\n",
    "#plot\n",
    "plt.figure(figsize=(6,8))\n",
    "plt.scatter(coordinates[:, 0], coordinates[:, 1], c=spatial_groups,\n",
    "            s=50, cmap='viridis');\n",
    "plt.title('Spatial clusters of training data')\n",
    "plt.ylabel('y')\n",
    "plt.xlabel('x');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you're happy with the spatial grouping, run the cell below to split your dataset into a testing and training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate n_splits of train-test_split\n",
    "cv_inner = SKCV(X=model_input[:, model_col_indices],\n",
    "                     y=model_input[:, 0],\n",
    "                     coordinates=coordinates,\n",
    "                     max_distance=max_distance,\n",
    "                     n_groups=n_clusters,\n",
    "                     n_splits=cv_splits,\n",
    "                     cluster_method=cluster_method,\n",
    "                     kfold_method=kfold_method,\n",
    "                     test_size=test_size,\n",
    "                     balance=balance\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n clusters = 111\n",
      "n clusters = 111\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "Accuracy: nan (nan)\n"
     ]
    }
   ],
   "source": [
    "X = model_input[:, model_col_indices]\n",
    "y = model_input[:, 0]\n",
    "# define the model\n",
    "model = RandomForestClassifier(random_state=1)\n",
    "# define search space\n",
    "cv_outer = SKCV(\n",
    "    X=model_input[:, model_col_indices],\n",
    "    y=model_input[:, 0],\n",
    "    coordinates=coordinates,\n",
    "    max_distance=max_distance,\n",
    "    n_groups=n_clusters,\n",
    "    n_splits=cv_splits,\n",
    "    cluster_method=cluster_method,\n",
    "    kfold_method=kfold_method,\n",
    "    test_size=test_size,\n",
    "    balance=balance,\n",
    ")\n",
    "\n",
    "# Create the parameter grid based on the results of random search\n",
    "param_grid = {\n",
    "    \"class_weight\": [\"balanced\", None],\n",
    "    \"max_features\": [\"auto\", \"log2\", None],\n",
    "    \"n_estimators\": [100, 200, 300, 400],\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "}\n",
    "# define search\n",
    "search = GridSearchCV(\n",
    "    model, param_grid, scoring=\"accuracy\", n_jobs=-1, cv=list(cv_inner), refit=True\n",
    ")\n",
    "\n",
    "# configure the cross-validation procedure\n",
    "cv_outer = SKCV(\n",
    "    X=model_input[:, model_col_indices],\n",
    "    y=model_input[:, 0],\n",
    "    coordinates=coordinates,\n",
    "    max_distance=max_distance,\n",
    "    n_groups=n_clusters,\n",
    "    n_splits=cv_splits,\n",
    "    cluster_method=cluster_method,\n",
    "    kfold_method=kfold_method,\n",
    "    test_size=test_size,\n",
    "    balance=balance,\n",
    ")\n",
    "\n",
    "# execute the nested cross-validation\n",
    "scores = cross_val_score(search, X, y, scoring=\"accuracy\", cv=list(cv_outer), n_jobs=-1)\n",
    "print(scores)\n",
    "# report performance\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "print(\"Accuracy: %.3f (%.3f)\" % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(random_state=1)    \n",
    "space = dict()\n",
    "space['n_estimators'] = [10, 100, 500]\n",
    "space['max_features'] = [2, 4, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features, train_labels, test_labels = spatial_train_test_split(X=model_input[:, model_col_indices],\n",
    "                                                             y=model_input[:, 0],\n",
    "                                                             coordinates=coordinates,\n",
    "                                                             max_distance=max_distance,\n",
    "                                                             n_groups=n_clusters,\n",
    "                                                             cluster_method=cluster_method,\n",
    "                                                             kfold_method=kfold_method,\n",
    "                                                             test_size=test_size,\n",
    "                                                             balance=balance\n",
    "                                                            )\n",
    "\n",
    "print(\"train_features shape:\", train_features.shape)\n",
    "print(\"test_features shape:\", test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Feature scaling\n",
    "\n",
    "Feature scaling (standardisaton or normalisation) of datasets is a common requirement for many machine learning estimators. For example, the objective function within the RBF kernel of Support Vector Machines assumes that all features are centered around zero and have variance in the same order. If a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly.\n",
    "\n",
    "Below, we demonstrate the use of a common preprocessing function provided by [sklearn](https://scikit-learn.org/stable/modules/preprocessing.html), `StandardScaler`, which will standardise the values in an array to the array's mean and standard deviation via the formuala: `z = (x-u/s)`, where `u` is the mean of and `s` is the standard deviation.\n",
    "\n",
    "To centre all values around 0, set `with_mean = False` within the `StandardScalar()` method\n",
    "\n",
    "> **Note**: <ins>Always</ins> split your data into a training and test set **BEFORE** feature scaling (normalisation or standardisation) is applied. This is because the test set is intended to be a completly independent set of data. If feature scaling is applied before splitting, then the data will share a common mean and standard deviation (i.e. **information leakage** occurs between the training and test datasets).\n",
    "\n",
    "> **Note**: <ins>Do not</ins> apply standardisation to categorical data (dummy variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instatiate a standard scalar method\n",
    "sc = StandardScaler()\n",
    "\n",
    "#apply standard scalar params to the training set\n",
    "train_features = sc.fit_transform(train_features)\n",
    "\n",
    "#apply the same standard scalar params to the test set\n",
    "test_features = sc.fit_transform(test_features)\n",
    "\n",
    "# apply the same standard scalar params to all the feature data, this is for our hyperparameter\n",
    "# searching later on which relies on the full dataset.\n",
    "model_input[:, model_col_indices] = sc.fit_transform(model_input[:, model_col_indices])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to export the standard scalar values for use in our classification in the subseqeuent script `Predict.ipnyb`, as these same values are applied to our prediction dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(sc, 'results/std_scaler.bin', compress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train default classifier\n",
    "\n",
    "The intial model will rely on the default parameters, during hyperparameter tuning later we will refine these parameters to see if we can improve on the default model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier(random_state=1, n_jobs=ncpus, verbose=0)\n",
    "model.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Classifier\n",
    "\n",
    "The following cells will help you examine the classifier and improve the results.  We can do this by:\n",
    "* Calculating the `cross-validation scores`, producing a `classification report` and a `confusion matrix`\n",
    "* Finding out which feature layers (bands in the input data) are most useful for classifying, and which are not,\n",
    "* Evaluating which model parameters (hyperparameters) optimize the model results \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy metrics\n",
    "\n",
    "We can use the sample of test data we partitioned earlier to test the accuracy of the trained model on this new, \"unseen\" data.\n",
    "\n",
    "We can also put our training data through a k-fold cross validation procedure as an additional way to test the model's accuracy and variance.  We will use a special version of k-fold cross validation written specially for handling spatial data, the deafrica function `SKCV` (Spatial K-Fold Cross Validation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict on the test dataset\n",
    "# predictions = model.predict(test_features)\n",
    "\n",
    "#generate n_splits of train-test_split\n",
    "ss = SKCV(X=model_input[:, model_col_indices],\n",
    "             y=model_input[:, 0],\n",
    "             coordinates=coordinates,\n",
    "             max_distance=max_distance,\n",
    "             n_groups=n_clusters,\n",
    "             n_splits=cv_splits,\n",
    "             cluster_method=cluster_method,\n",
    "             kfold_method=kfold_method,\n",
    "             test_size=test_size,\n",
    "             balance=balance\n",
    "            )\n",
    "\n",
    "# cross validate accounting for spatial groups\n",
    "score = cross_val_score(RandomForestClassifier(),\n",
    "                        model_input[:, model_col_indices],\n",
    "                        model_input[:, 0],\n",
    "                        cv=ss,\n",
    "                        scoring=metric\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"=== Confusion Matrix ===\")\n",
    "# print(confusion_matrix(test_labels, predictions))\n",
    "# print('\\n')\n",
    "# print(\"=== Overall Accuracy ===\")\n",
    "# accuracy = accuracy_score(test_labels, predictions)\n",
    "# print(round(accuracy, 3))\n",
    "# print('\\n')\n",
    "# print(\"=== Classification Report ===\")\n",
    "# print(classification_report(test_labels, predictions))\n",
    "# print('\\n')\n",
    "print(\"=== Spatial K-Fold Cross-Val \"+metric+\" Scores ===\")\n",
    "print(np.array_str(score, precision=2, suppress_small=True))\n",
    "print(\"Mean: \"+ str(round(score.mean(), 2)))\n",
    "print(\"Std: \"+ str(round(score.std(), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine Feature Importance\n",
    "\n",
    "Extract classifier estimates of the relative importance of each band/variable for training the classifier. Useful for potentially selecting a subset of input bands/variables for model training/classification (i.e. optimising feature space). Results will be presented in descending order with the most important features listed first.  Importance is reported as a relative fraction between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shows the feature importance of the input features for predicting the class labels provided\n",
    "order = np.argsort(model.feature_importances_)\n",
    "\n",
    "plt.figure(figsize=(40,5))\n",
    "plt.bar(x=np.array(column_names[1:])[order],\n",
    "        height=model.feature_importances_[order])\n",
    "plt.gca().set_ylabel('Importance', labelpad=15)\n",
    "plt.gca().set_xlabel('Variable', labelpad=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize hyperparameters\n",
    "\n",
    "Hyperparameter searches are a required process in machine learning. Machine learning models require certain “hyperparameters”, model parameters that can be learned from the data. Finding these good values for these parameters is a “hyperparameter search” or an “hyperparameter optimization.”\n",
    "\n",
    "To optimize the parameters in our model, we use [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) to exhaustively search through a set of parameters and determine the combination that will result in the highest accuracy based upon the accuracy metric defined.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search with Cross Validation\n",
    "\n",
    "\n",
    "> **Note**: the parameters in the `param_grid` object depend on the classifier being used. The default example is set up for a random forest classifier, to adjust ther paramaters to suit a different classifier, look up the important parameters under the relevant [sklearn documentation](https://scikit-learn.org/stable/supervised_learning.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'class_weight': ['balanced', None],\n",
    "    'max_features': ['auto', 'log2', None],\n",
    "    'n_estimators': [100,200,300,400],\n",
    "    'criterion':['gini', 'entropy']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate n_splits of train-test_split\n",
    "ss = SKCV(X=model_input[:, model_col_indices],\n",
    "             y=model_input[:, 0],\n",
    "             coordinates=coordinates,\n",
    "             max_distance=max_distance,\n",
    "             n_groups=n_clusters,\n",
    "             n_splits=cv_splits,\n",
    "             cluster_method=cluster_method,\n",
    "             kfold_method=kfold_method,\n",
    "             test_size=test_size,\n",
    "             balance=balance\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 48 candidates, totalling 480 fits\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X must have exactly 2 columns (21 given).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-836c14bbe7db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m                    n_jobs=ncpus)\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_col_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The most accurate combination of tested parameters is: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/env/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/env/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/env/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1188\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/env/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    713\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 715\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/deafrica-sandbox-notebooks/Scripts/deafrica_classificationtools.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   1275\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m             raise ValueError(\"X must have exactly 2 columns ({} given).\".format(\n\u001b[0;32m-> 1277\u001b[0;31m                 X.shape[1]))\n\u001b[0m\u001b[1;32m   1278\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: X must have exactly 2 columns (21 given)."
     ]
    }
   ],
   "source": [
    "#instatiate a gridsearchCV\n",
    "clf = GridSearchCV(RandomForestClassifier(),\n",
    "                   param_grid,\n",
    "                   scoring=metric,\n",
    "                   verbose=1,\n",
    "                   cv=ss,\n",
    "                   n_jobs=ncpus)\n",
    "\n",
    "clf.fit(model_input[:, model_col_indices], model_input[:, 0])\n",
    "\n",
    "print(\"The most accurate combination of tested parameters is: \")\n",
    "pprint(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain and re-evaluate model\n",
    "\n",
    "Using the best parameters from our hyperparmeter optmization search, we now re-run our model and re-test its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new model\n",
    "new_model = Classifier(**clf.best_params_, random_state=1, n_jobs=ncpus)\n",
    "new_model.fit(train_features, train_labels)\n",
    "\n",
    "#generate new predictions\n",
    "predictions = new_model.predict(test_features)\n",
    "\n",
    "#generate n_splits of train-test_split\n",
    "ss = SKCV(X=model_input[:, model_col_indices],\n",
    "             y=model_input[:, 0],\n",
    "             coordinates=coordinates,\n",
    "             max_distance=max_distance,\n",
    "             n_groups=n_clusters,\n",
    "             n_splits=cv_splits,\n",
    "             cluster_method=cluster_method,\n",
    "             kfold_method=kfold_method,\n",
    "             test_size=test_size,\n",
    "             balance=balance\n",
    "            )\n",
    "\n",
    "#generate metrics\n",
    "new_score = cross_val_score(model,\n",
    "                        model_input[:, model_col_indices],\n",
    "                        model_input[:, 0],\n",
    "                        cv=ss,\n",
    "                        scoring=metric\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Confusion Matrix ===\")\n",
    "print(confusion_matrix(test_labels, predictions))\n",
    "print('\\n')\n",
    "print(\"=== Overall Accuracy ===\")\n",
    "new_accuracy = accuracy_score(test_labels, predictions)\n",
    "print(round(new_accuracy, 2))\n",
    "print('\\n')\n",
    "print(\"=== Classification Report ===\")\n",
    "print(classification_report(test_labels, predictions))\n",
    "print('\\n')\n",
    "print(\"=== Spatial K-Fold Cross-Val \"+metric+\" Scores ===\")\n",
    "print(np.array_str(new_score, precision=2, suppress_small=True))\n",
    "print(\"Mean: \"+ str(round(new_score.mean(), 2)))\n",
    "print(\"Std: \"+ str(round(new_score.std(), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Improvement in Spatial K-Fold Cross-Val \"+metric+\" score:\")\n",
    "print(round(new_score.mean() - score.mean(),4))\n",
    "print('\\n')\n",
    "print(\"Improvement in overall accuracy:\")\n",
    "print(round(new_accuracy - accuracy,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Receiver Operating Characteristic (ROC) Curve\n",
    "\n",
    "> **Note** :This is for binary classifications only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final metric we use to evaluate our classifier is to plot the Receiver Operating Characteristic (ROC) curve. An ROC curve is an excellent method of measuring the performance of a classification model. The True Positive Rate (TPR) is plot against the False Positive Rate (FPR) for the probabilities of a classifier's predictions. The area under the plot is calculated.\n",
    "\n",
    "**The higher the area under the curve (AUC), the better the model is at distinguishing the classes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize = (16, 8))\n",
    "\n",
    "#Default model\n",
    "axes[0].plot([0,1], [0,1], 'r--')\n",
    "probs = model.predict_proba(test_features)\n",
    "# Reading probability of second class\n",
    "probs = probs[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(test_labels, probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "label = 'Classifier AUC:' + ' {0:.2f}'.format(roc_auc)\n",
    "axes[0].plot(fpr, tpr, c = 'g', label = label, linewidth = 4)\n",
    "axes[0].set_xlabel('False Positive Rate', fontsize = 10)\n",
    "axes[0].set_ylabel('True Positive Rate', fontsize = 10)\n",
    "axes[0].set_title('Default Model Receiver Operating Characteristic', fontsize = 10)\n",
    "axes[0].legend(loc = 'lower right', fontsize = 10)\n",
    "\n",
    "#Optimized model\n",
    "axes[1].plot([0,1], [0,1], 'r--')\n",
    "probs = new_model.predict_proba(test_features)\n",
    "# Reading probability of second class\n",
    "probs = probs[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(test_labels, probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "label = 'Classifier AUC:' + ' {0:.3f}'.format(roc_auc)\n",
    "axes[1].plot(fpr, tpr, c = 'g', label = label, linewidth = 4)\n",
    "axes[1].set_xlabel('False Positive Rate', fontsize = 10)\n",
    "axes[1].set_ylabel('True Positive Rate', fontsize = 10)\n",
    "axes[1].set_title('Optimized Model Receiver Operating Characteristic', fontsize = 10)\n",
    "axes[1].legend(loc = 'lower right', fontsize = 10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model\n",
    "\n",
    "Running this cell will export the classifier as a binary`.joblib` file. This will allow for importing the model in the subsequent script, `4_Predict.ipynb` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(new_model, 'results/ml_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommended next steps\n",
    "\n",
    "To continue working through the notebooks in this `Scalable Machine Learning on the ODC` workflow, go to the next notebook `4_Predict.ipynb`.\n",
    "\n",
    "1. [Extracting_training_data](1_Extracting_training_data.ipynb) \n",
    "2. [Inspect_training_data](2_Inspect_training_data.ipynb)\n",
    "3. **Train_fit_evaluate_classifier**\n",
    "4. [Predict](4_Predict.ipynb)\n",
    "5. [Accuracy_assessment](5_Accuracy_assessment.ipynb)\n",
    "6. [Object-based_filtering](6_Object-based_filtering_(optional).ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Additional information\n",
    "\n",
    "**License:** The code in this notebook is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \n",
    "Digital Earth Africa data is licensed under the [Creative Commons by Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "\n",
    "**Contact:** If you need assistance, please post a question on the [Open Data Cube Slack channel](http://slack.opendatacube.org/) or on the [GIS Stack Exchange](https://gis.stackexchange.com/questions/ask?tags=open-data-cube) using the `open-data-cube` tag (you can view previously asked questions [here](https://gis.stackexchange.com/questions/tagged/open-data-cube)).\n",
    "If you would like to report an issue with this notebook, you can file one on [Github](https://github.com/digitalearthafrica/deafrica-sandbox-notebooks).\n",
    "\n",
    "**Last modified:** August 2020\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tags\n",
    "Browse all available tags on the DE Africa User Guide's [Tags Index](https://) (placeholder as this does not exist yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
